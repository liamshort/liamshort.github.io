[{"content":"Overview There\u0026rsquo;s no easy way to retrieve the total volume of data replicated via MGN, so i wrote a wee script.\nThe below Python script fetches data on all source servers replicated to MGN, including disk sizes. It then parses the data and outputs to a CSV file in the local directory.\nUsers can define in the following parameters:\nParameter Description Default region The AWS Region eu-west-1 output Name of the CSV output file mgn_data exclude List of Source Server IDs to exclude [] max Maximum number of Source Servers to Parse 100 The script can be found here.\nPreqrequisites Using Python3 Assumed into the target AWS account with an identity that permits the mgn:DescribeSourceServers action Usage gh repo clone liamshort/mgn-parser-tool cd mgn-parser-tool pip install -r requirements.txt python script.py -r \u0026lt;REGION\u0026gt; ","permalink":"http://liamshort.dev/posts/mgn-replication-stats/","summary":"Overview There\u0026rsquo;s no easy way to retrieve the total volume of data replicated via MGN, so i wrote a wee script.\nThe below Python script fetches data on all source servers replicated to MGN, including disk sizes. It then parses the data and outputs to a CSV file in the local directory.\nUsers can define in the following parameters:\nParameter Description Default region The AWS Region eu-west-1 output Name of the CSV output file mgn_data exclude List of Source Server IDs to exclude [] max Maximum number of Source Servers to Parse 100 The script can be found here.","title":"Get MGN Replication Statistics"},{"content":"The Module This is a simple Terraform module to deploy an IAM Role which is assumable by GitHub Actions for a specific user or organisation. The module can be found here.\nTo consume the module, invoke as shown below:\nmodule \u0026#34;github\u0026#34; { source = \u0026#34;github.com/liamshort/tf-mod-github-oidc-role\u0026#34; name_prefix = \u0026#34;MY_NAME_PREFIX\u0026#34; github_username = \u0026#34;MY_GITHUB_USERNAME\u0026#34; github_thumbprint = \u0026#34;6938fd4d98bab03faadb97b34396831e3780aea1\u0026#34; } Getting the Thumbprint The process for retrieving the Thumbprint can be found here.\nRun:\nopenssl s_client -servername token.actions.githubusercontent.com -showcerts -connect token.actions.githubusercontent.com:443 Copy the cert 1 (from -----BEGIN CERTIFICATE----- to -----END CERTIFICATE-----), not 0 and paste into the following local file certificate.crt. Then Run:\nopenssl x509 -in certificate.crt -fingerprint -noout | cut -f2 -d\u0026#39;=\u0026#39; | tr -d \u0026#39;:\u0026#39; | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39; The resulting string is the Thumbprint for GitHub OIDC.\nLinks More information on GitHub OIDC and Cloud Providers can be found here.\nAs noted in their blog post, it is possible for the GitHub thumbprint to change. If this changes, users must update the thumprint to maintain access to AWS.\n","permalink":"http://liamshort.dev/posts/github-oicd-aws/","summary":"The Module This is a simple Terraform module to deploy an IAM Role which is assumable by GitHub Actions for a specific user or organisation. The module can be found here.\nTo consume the module, invoke as shown below:\nmodule \u0026#34;github\u0026#34; { source = \u0026#34;github.com/liamshort/tf-mod-github-oidc-role\u0026#34; name_prefix = \u0026#34;MY_NAME_PREFIX\u0026#34; github_username = \u0026#34;MY_GITHUB_USERNAME\u0026#34; github_thumbprint = \u0026#34;6938fd4d98bab03faadb97b34396831e3780aea1\u0026#34; } Getting the Thumbprint The process for retrieving the Thumbprint can be found here.\nRun:\nopenssl s_client -servername token.","title":"GitHub Actions and OICD with AWS"},{"content":"Overview This website uses custom APIs hosted in AWS, built using API Gateway and backend Lambda Functions. To reduce the time taken for development and testing, I recently started using the AWS Toolkit extension for Visual Studio Code, which allows users to interact directly with the following services from their IDE:\nAPI Gateway CloudFormation Cloudwatch Logs ECR ECS IoT Lambda S3 Step Functions Systems Manager Although it is possible to interact with these services via the AWS API, the extension does provide a nice UI.\nMore information on the AWS Toolkit for Visual Studio Code for AWS and Microsoft.\nInstallation Installation of the extension is easy. Within Visual Studio Code \u0026gt; Extensions \u0026gt; Search for AWS Toolkit \u0026gt; Install. Once installed, open the extension from the AWS icon in the same panel as the Extensions icon. To authenticate, users can choose an existing profile from their config file (~/.aws/config). Once authenticated, users can start interacting with services in their AWS account.\nUsage Store Lambda test payloads alongside your application code, this ensures the test payloads are easily accessible and version controlled. To test a Lambda Function, open up the toolkit \u0026gt; Lambda \u0026gt; right click on my function \u0026gt; Invoke on AWS \u0026gt; either paste the payload or upload from file \u0026gt; Invoke. The response will be returned in the OUTPUT console of Visual Studio Code. For example, when testing my Wordle Helper API, I can use the following payload:\n{ \u0026#34;rawQueryString\u0026#34;: \u0026#34;wordle?found=bea\u0026amp;exclude=\u0026amp;letter0=b\u0026amp;letter1=e\u0026amp;letter2=\u0026amp;letter3=\u0026amp;letter4=\u0026#34; } If CloudWatch Logs are permitted for the Lambda Function, users can view these by opening up the toolkit \u0026gt; CloudWatch Logs \u0026gt; right click on a Log Group \u0026gt; View Log Stream... \u0026gt; select a Log Stream.\nAs some of my APIs interact with S3, I can check the contents of a bucket by going into the toolkit \u0026gt; S3 \u0026gt; select a bucket \u0026gt; select an object. Most filetypes can be viewed using the toolkit, including images.\nIn addition to interacting with specific AWS services, the toolkit allows users to view the configuration of their AWS resources. Go to the toolkit \u0026gt; Resources \u0026gt; select options (the gear) \u0026gt; select a resource (such as S3 bucket, CloudFront Distribution, SNS Topic etc\u0026hellip;) \u0026gt; once loaded select the resource instance from the dropdown. Users can now view the configuration of specific resource instances in JSON.\n","permalink":"http://liamshort.dev/posts/aws-toolkit-vscode/","summary":"Overview This website uses custom APIs hosted in AWS, built using API Gateway and backend Lambda Functions. To reduce the time taken for development and testing, I recently started using the AWS Toolkit extension for Visual Studio Code, which allows users to interact directly with the following services from their IDE:\nAPI Gateway CloudFormation Cloudwatch Logs ECR ECS IoT Lambda S3 Step Functions Systems Manager Although it is possible to interact with these services via the AWS API, the extension does provide a nice UI.","title":"AWS Toolkit Extension for VSCode"},{"content":"Overview I recently wanted to clone all my repositories from GitHub, easy right? Well, I couldn\u0026rsquo;t find a nice way around it. Apparently there\u0026rsquo;s no built in GitHub functionality for this. So, i tried to write my own Python script to clone all repos via the GitHub API. No Joy. I then stumbled upon the GitHub CLI.\nInstallation The GitHub CLI can be downloaded on a Mac or Linux machine with brew:\nbrew install gh Once installed users can run through the initial setup by running the following and authenticating to GitHub:\ngh auth login I used a Personal Access Token to authenticate with the above.\nBulk Operations Once we have the CLI downloaded and initialised, we can use the power of bash to loop through all our repositories and perform actions in bulk.\nTo clone all repositories for a personal account, run:\ngh repo list MY_USERNAME | while read -r repo _; do gh repo clone \u0026#34;$repo\u0026#34; done To break the above command down, we\u0026rsquo;re first listing all repositories for our user, loop through each of the returned repositories, cloning each as we loop through.\nTo delete all repositories for a personal account (DANGEROUS), run:\ngh repo list MY_USERNAME | while read -r repo _; do gh repo delete \u0026#34;$repo\u0026#34; --confirm done Viewing Workflows To view GitHub Actions workflows for a local repository, run:\ngh workflow view To check a non local repository, run:\ngh workflow view --repo MY_REPO_NAME ","permalink":"http://liamshort.dev/posts/github-cli/","summary":"Overview I recently wanted to clone all my repositories from GitHub, easy right? Well, I couldn\u0026rsquo;t find a nice way around it. Apparently there\u0026rsquo;s no built in GitHub functionality for this. So, i tried to write my own Python script to clone all repos via the GitHub API. No Joy. I then stumbled upon the GitHub CLI.\nInstallation The GitHub CLI can be downloaded on a Mac or Linux machine with brew:","title":"GitHub CLI"},{"content":"Overview The website thispersondoesnotexist.com is amazing, displaying a truly lifelike image of someone who does not exist. Developer by Nividia, this technology was intended for game design. The underlying code for the face generation can be found at SyleGen.\nUpon seeing this website back in 2019, the first thing that came to my mind was a random profile generator. What if we could associate some realistic details with a completely non-existant person, making it very tricky to discern if the person actually exists. To accomplish this, I set out to build an API that would return a randomly generated profile.\nThe API backend comprises of two Lambda Functions, the first is triggered on a schedule to generate profiles, the second queries the database and returns data to the website. The Lambda Function which generates the profile, first queiries the thispersondoesnotexist.com to retrieve our random face. The bytes from this image are passed into the AWS Rekognition service to be analysed, returning the estimated gender and an age range of the face. The age of the face is a random number from the low and high ends of the age range.\nWe now have an image of a face, a gender and an age. The randomuser.me API is queried, passing in the estimated gender, which returns a profile consisting of the following useful details:\nfirst name middle name last name residence Street City State / County Postcode Country TimeZone email address nationality Further information can be found in their API documentation.\nThe image is then uploaded to an S3 Bucket, with a corresponding entry made in a DynamoDB Table. This entry contains the URL of the image along with the name, place of residence and email address of our fictional friend. To facilitate cleanup of old profiles, the S3 Bucket has an associated lifecycle policy to delete objects after 1 day and the DynamoDB Table uses a ttl key to delete items after 1 day. This ensures that generated profiles are not retained for more than 1 day.\nOriginally the API would return a newly generated profile upon each invocation. However, to avoid getting spammed with profile requests, the Lambda Function which generates new profiles is scheduled to run every four hours by an EventBridge Rule. This ensures that the profiles on display are frequently updated.\nMy website at liamshort.dev uses JavaScript to invoke the API, which quieries the DynamoDB Table and returns the content to the user.\n","permalink":"http://liamshort.dev/posts/random-profile-generator/","summary":"Overview The website thispersondoesnotexist.com is amazing, displaying a truly lifelike image of someone who does not exist. Developer by Nividia, this technology was intended for game design. The underlying code for the face generation can be found at SyleGen.\nUpon seeing this website back in 2019, the first thing that came to my mind was a random profile generator. What if we could associate some realistic details with a completely non-existant person, making it very tricky to discern if the person actually exists.","title":"Random Profile Generator"},{"content":"TL;DR Manage Git Repositories using the following Terraform module for GitHub.\nOverview Just had a great idea for a new project and want to get coding ASAP? The first task is often setting up a Git repository to host the code. If you\u0026rsquo;re like me and love to tinkerer, you can quickly accumulate repositories which makes things feel cluttered! Why not manage your repositories via Terraform?\nTerraform, the open source Infrastructure as Code (IaC) tool developed by Hashicorp. Terraform uses Hashicorp Configuration Language (HCL) to define and deploy all manner of things and allows users to create and consume Providers, which are plugins for services like AWS, GCP, Azure, DigitalOcean. You can view all the Official, Verified and community Providers here. Terraform also allows users to create and share their own Modules, bundles of Terraform code designed to perform a specific task. For example, if we needed a CICD pipeline for AWS, we could use this module created by CloudPosse. For more information on Terraform modules, look here.\nMy first real exposure to Terraform was in 2020, when preparing for a new client engagement I got my Hashicorp Terraform Associate Certification. Before this time I had been working mostly with AWS CloudFormation, which in comparison now feels clunky. To name a few of the features which makes Terraform attractive to me:\nCloud agnostic, meaning it can be used across all the major CSPs with no vendor lock in HCL is logical, it just makes sense Modules are insanely useful State management is controlled by the user (glad I haven\u0026rsquo;t touched a CloudFormation Stack in years) GitHub is the most popular Git provider, synonymous with Git itself. We can use the GitHub Provider for Terraform to create and manage various things like Repositories, Actions and User / Organization permissions.\nThe Module I put together a simple Terraform Module which can be used to manage Git repositores for GitHub (the same logic can be applied for other Git management tools such as CodeCommit or Gitea):\nGitHub Usage The modules all follow the same configuration, containing a config.yml file which contains the definitions of our repositories in HCL. Based on the contents of this file we can create, update or delete repositires. Below is an example of a GitHub repository definition from the config.yml:\nmy-super-cool-repo: description: Code glorious code visibility: public topics: [ \u0026#34;stuff\u0026#34; ] has_wiki: true has_projects: true has_issues: true vulnerability_alerts: true Importing We can import existing repositories that were created manually, to do this add a definition to the config.yml which matches the configuration of the existing repository. The repository can then be imported to the Terraform state by running the terraform import command, as shown below:\nterraform import \u0026#39;module.github.github_repository.this[\u0026#34;my-super-cool-repo\u0026#34;]\u0026#39; my-super-cool-repo Voila! Your repo can now be managed using Terraform. After the import run a terraform plan to confirm what differences (if any) Terraform has detected between your code and the actual repository configuration. Terraform will try to resolve these if we run a subsequent terraform apply.\n","permalink":"http://liamshort.dev/posts/terraform-github-repositories/","summary":"TL;DR Manage Git Repositories using the following Terraform module for GitHub.\nOverview Just had a great idea for a new project and want to get coding ASAP? The first task is often setting up a Git repository to host the code. If you\u0026rsquo;re like me and love to tinkerer, you can quickly accumulate repositories which makes things feel cluttered! Why not manage your repositories via Terraform?\nTerraform, the open source Infrastructure as Code (IaC) tool developed by Hashicorp.","title":"Terraform and Git Repositories"},{"content":"","permalink":"http://liamshort.dev/tags/","summary":"","title":"Tags"}]